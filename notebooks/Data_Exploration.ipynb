{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c03c201",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing\n",
    "## Exploratory Data Analysis (EDA), Feature Correlation, and Preprocessing\n",
    "\n",
    "This notebook covers the essential steps for understanding and preparing economic data for modeling:\n",
    "- **Exploratory Data Analysis (EDA)**: Uncover patterns, anomalies, and insights.\n",
    "- **Feature Engineering**: Create new variables to improve model performance.\n",
    "- **Correlation Analysis**: Understand relationships between variables.\n",
    "- **Data Preprocessing**: Prepare data for machine learning models.\n",
    "\n",
    "### Key Features:\n",
    "- Comprehensive summary statistics and data quality checks.\n",
    "- Advanced visualizations for distribution, correlation, and time-series analysis.\n",
    "- Interactive plots for deep-dive analysis.\n",
    "- Standard preprocessing pipelines (scaling, transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2129fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "print(\"üìö Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf0cee",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection\n",
    "\n",
    "Load the dataset and perform an initial high-level inspection to understand its structure, data types, and basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (can be overridden by Streamlit)\n",
    "correlation_threshold = 0.7\n",
    "outlier_threshold = 2.5\n",
    "\n",
    "# Load data from a sample file (replace with actual data source)\n",
    "def load_sample_data():\n",
    "    \"\"\"Load sample economic data\"\"\"\n",
    "    # In a real scenario, load from data/raw or data/cleaned\n",
    "    # For this notebook, we generate synthetic data\n",
    "    dates = pd.date_range('2010-01-01', periods=120, freq='M')\n",
    "    data = pd.DataFrame({\n",
    "        'GDP': 1000 + np.arange(120) * 10 + np.random.normal(0, 20, 120),\n",
    "        'Inflation': 2 + np.sin(np.arange(120) / 12) * 0.5 + np.random.normal(0, 0.2, 120),\n",
    "        'Unemployment': 5 - np.cos(np.arange(120) / 24) * 1.5 + np.random.normal(0, 0.3, 120),\n",
    "        'Interest_Rate': 1.5 + 0.5 * (2 + np.sin(np.arange(120) / 12) * 0.5) + np.random.normal(0, 0.1, 120),\n",
    "        'Public_Debt': 500 * np.exp(np.cumsum(np.random.normal(0.01, 0.005, 120))),\n",
    "        'Trade_Balance': -10 + 20 * np.sin(np.arange(120) / 12 + np.pi/4) + np.random.normal(0, 8, 120),\n",
    "        'Region': np.random.choice(['North', 'South', 'East', 'West'], 120)\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Add some missing values and outliers\n",
    "    for col in ['GDP', 'Inflation']:\n",
    "        data.loc[data.sample(frac=0.05).index, col] = np.nan\n",
    "    data.loc[data.sample(1).index, 'Unemployment'] = 20 # Outlier\n",
    "    \n",
    "    return data\n",
    "\n",
    "df = load_sample_data()\n",
    "\n",
    "print(\"üìä DATA OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape of data: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\nüìã FIRST 5 ROWS:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n‚ÑπÔ∏è DATA INFO:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a14d1a",
   "metadata": {},
   "source": [
    "## 2. Data Quality and Cleaning\n",
    "\n",
    "Assess data quality by checking for missing values, duplicates, and inconsistencies. Apply cleaning techniques to prepare the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "\n",
    "print(\"üóëÔ∏è MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(missing_df[missing_df['Missing Values'] > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "fig = px.imshow(df.isnull(), title='Missing Value Heatmap', color_continuous_scale='gray_r')\n",
    "fig.show()\n",
    "\n",
    "# Handle missing values (imputation)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(\"\\n‚úÖ Missing values handled using mean imputation.\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nüìã DUPLICATE ROWS: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(\"‚úÖ Duplicate rows removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e12775",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Perform EDA to understand the distribution of each variable, identify trends, and uncover relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdcd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"üìä SUMMARY STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(df.describe().round(2))\n",
    "\n",
    "# Distribution of numeric variables\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) - 1) // n_cols + 1\n",
    "\n",
    "fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=numeric_cols)\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    row = i // n_cols + 1\n",
    "    col_pos = i % n_cols + 1\n",
    "    fig.add_trace(go.Histogram(x=df[col], name=col, nbinsx=30), row=row, col=col_pos)\n",
    "\n",
    "fig.update_layout(title='Distribution of Economic Indicators', height=300*n_rows, showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Time series plots\n",
    "fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=numeric_cols)\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    row = i // n_cols + 1\n",
    "    col_pos = i % n_cols + 1\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df[col], name=col, mode='lines'), row=row, col=col_pos)\n",
    "\n",
    "fig.update_layout(title='Time Series of Economic Indicators', height=300*n_rows, showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0aca81",
   "metadata": {},
   "source": [
    "## 4. Correlation and Relationship Analysis\n",
    "\n",
    "Analyze the correlation between variables to identify multicollinearity and understand key relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78feed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "print(\"üîó CORRELATION MATRIX\")\n",
    "print(\"=\"*50)\n",
    "print(corr_matrix.round(2))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig = px.imshow(corr_matrix, \n",
    "                title='Correlation Matrix of Economic Indicators',\n",
    "                color_continuous_scale='RdBu_r',\n",
    "                zmin=-1, zmax=1,\n",
    "                text_auto=True)\n",
    "fig.update_layout(height=600)\n",
    "fig.show()\n",
    "\n",
    "# Identify highly correlated pairs\n",
    "high_corr_pairs = corr_matrix.abs().unstack().sort_values(ascending=False)\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs < 1]\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs > correlation_threshold]\n",
    "\n",
    "print(f\"\\nüéØ HIGHLY CORRELATED PAIRS (Threshold > {correlation_threshold}):\")\n",
    "print(high_corr_pairs.head(10))\n",
    "\n",
    "# Pair plot for detailed relationship analysis\n",
    "print(\"\\nüìà Generating pair plot (this may take a moment)...\")\n",
    "fig = px.scatter_matrix(df, dimensions=numeric_cols, \n",
    "                        title='Pair Plot of Economic Indicators',\n",
    "                        color='Region')\n",
    "fig.update_layout(height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25e958",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Create new features from existing data to enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e27087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõ†Ô∏è FEATURE ENGINEERING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_eng = df.copy()\n",
    "\n",
    "# Time-based features\n",
    "df_eng['Year'] = df_eng.index.year\n",
    "df_eng['Month'] = df_eng.index.month\n",
    "df_eng['Quarter'] = df_eng.index.quarter\n",
    "print(\"‚úÖ Added time-based features (Year, Month, Quarter)\")\n",
    "\n",
    "# Lag features\n",
    "for col in ['GDP', 'Inflation']:\n",
    "    for lag in [1, 3, 6]:\n",
    "        df_eng[f'{col}_lag_{lag}'] = df_eng[col].shift(lag)\n",
    "print(\"‚úÖ Added lag features for GDP and Inflation\")\n",
    "\n",
    "# Rolling window features\n",
    "for col in ['Unemployment', 'Interest_Rate']:\n",
    "    for window in [3, 6]:\n",
    "        df_eng[f'{col}_rolling_mean_{window}'] = df_eng[col].rolling(window=window).mean()\n",
    "        df_eng[f'{col}_rolling_std_{window}'] = df_eng[col].rolling(window=window).std()\n",
    "print(\"‚úÖ Added rolling window features for Unemployment and Interest Rate\")\n",
    "\n",
    "# Interaction features\n",
    "df_eng['Debt_to_GDP'] = df_eng['Public_Debt'] / df_eng['GDP']\n",
    "print(\"‚úÖ Added interaction feature: Debt_to_GDP ratio\")\n",
    "\n",
    "# Drop rows with NaNs created by feature engineering\n",
    "df_eng = df_eng.dropna()\n",
    "\n",
    "print(f\"\\nüìä Shape of engineered data: {df_eng.shape}\")\n",
    "print(\"\\nüìã ENGINEERED FEATURES PREVIEW:\")\n",
    "print(df_eng.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3087c8",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing\n",
    "\n",
    "Prepare the data for machine learning models through scaling, encoding, and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffa92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_processed = df_eng.copy()\n",
    "\n",
    "# 1. Encoding categorical variables\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "region_encoded = encoder.fit_transform(df_processed[['Region']])\n",
    "region_df = pd.DataFrame(region_encoded, columns=encoder.get_feature_names_out(['Region']), index=df_processed.index)\n",
    "df_processed = pd.concat([df_processed.drop('Region', axis=1), region_df], axis=1)\n",
    "print(\"‚úÖ Encoded 'Region' using One-Hot Encoding\")\n",
    "\n",
    "# 2. Scaling numerical features\n",
    "numeric_cols_to_scale = df_processed.select_dtypes(include=np.number).columns\n",
    "scaler = StandardScaler()\n",
    "df_processed[numeric_cols_to_scale] = scaler.fit_transform(df_processed[numeric_cols_to_scale])\n",
    "print(\"‚úÖ Scaled numerical features using StandardScaler\")\n",
    "\n",
    "print(\"\\nüìä PROCESSED DATA PREVIEW:\")\n",
    "print(df_processed.head().round(2))\n",
    "\n",
    "# 3. Principal Component Analysis (PCA) for dimensionality reduction\n",
    "pca = PCA(n_components=0.95) # Retain 95% of variance\n",
    "df_pca = pca.fit_transform(df_processed)\n",
    "\n",
    "print(f\"\\nüî¨ PCA ANALYSIS:\")\n",
    "print(f\"   Original number of features: {df_processed.shape[1]}\")\n",
    "print(f\"   Number of components to retain 95% variance: {pca.n_components_}\")\n",
    "\n",
    "# Visualize explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, len(explained_variance) + 1), y=explained_variance, mode='lines+markers'))\n",
    "fig.add_hline(y=0.95, line_dash='dash', line_color='red', annotation_text='95% Variance')\n",
    "fig.update_layout(title='PCA Explained Variance', xaxis_title='Number of Components', yaxis_title='Cumulative Explained Variance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c881cd",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "Key findings from the data exploration and preprocessing steps, and recommendations for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0635fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üìä DATA QUALITY:\")\n",
    "print(\"  - Initial dataset contained missing values, which were imputed.\")\n",
    "print(\"  - No duplicate rows were found after initial cleaning.\")\n",
    "\n",
    "print(\"üìà KEY INSIGHTS:\")\n",
    "print(\"  - Most economic indicators show clear time-dependent trends and seasonality.\")\n",
    "print(\"  - Strong correlation observed between Interest Rate and Inflation.\")\n",
    "  - 'Unemployment' variable contained a significant outlier, which should be handled during modeling.\")\n",
    "\n",
    "print(\"üõ†Ô∏è FEATURE ENGINEERING:\")\n",
    "print(\"  - Created time-based, lag, and rolling window features to capture temporal dynamics.\")\n",
    "print(\"  - Engineered 'Debt_to_GDP' ratio, a critical economic indicator.\")\n",
    "\n",
    "print(\"üîÑ PREPROCESSING:\")\n",
    "print(\"  - Categorical features were one-hot encoded.\")\n",
    "print(\"  - Numerical features were standardized to have zero mean and unit variance.\")\n",
    "print(\"  - PCA suggests that dimensionality can be significantly reduced while retaining most of the variance.\")\n",
    "\n",
    "print(\"\\nüí° NEXT STEPS & RECOMMENDATIONS:\")\n",
    "print(\"  1. **Modeling**: Use the preprocessed data to train predictive models.\")\n",
    "print(\"  2. **Feature Selection**: Use techniques like RFE or feature importance from tree-based models to select the most relevant features.\")\n",
    "print(\"  3. **Outlier Handling**: Implement robust scaling or outlier removal techniques before training sensitive models like linear regression.\")\n",
    "print(\"  4. **Cross-Validation**: Use time-series cross-validation (e.g., TimeSeriesSplit) to evaluate models robustly.\")\n",
    "\n",
    "print(\"\\n‚úÖ Data exploration and preprocessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
